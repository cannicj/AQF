{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "abf2d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score\n",
    "\n",
    "import tensorflow as tf  \n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "28a3836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/juliusraschke/Documents/Quantitative Finance/Summer Semester 2/Advanced Quant Finance/Data'\n",
    "\n",
    "VIX_TS = pd.read_csv(DATA_PATH + '/VIX_TS_CUSTOM.csv')\n",
    "\n",
    "VIX_TS = VIX_TS.rename({'Date':'DATE','VIX_1^2':'1 month','VIX_2^2':'2 month','VIX_3^2':'3 month','VIX_6^2':'6 month',\n",
    "               'VIX_9^2':'9 month','VIX_12^2':'12 month'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e184104",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "bbf9bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = VIX_TS.iloc[:,1:7]\n",
    "Y = VIX_TS['DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe1f55",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e887cd",
   "metadata": {},
   "source": [
    "### 1st Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "d488438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "7f3a4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reconstruction loss function\n",
    "def reconstruction_loss(data, recon_data, MSE=True):\n",
    "    if MSE:\n",
    "        return nn.MSELoss()(data, recon_data)\n",
    "    else:\n",
    "        return nn.BCELoss()(data, recon_data)\n",
    "\n",
    "# Define the covariance loss function (unchanged)\n",
    "def cov_loss(z, step):\n",
    "    if step > 1:\n",
    "        loss = 0\n",
    "        for idx in range(step - 1):\n",
    "            loss += ((z[:, idx] * z[:, -1]).mean()) ** 2\n",
    "        loss = loss / (step - 1)\n",
    "    else:\n",
    "        loss = torch.zeros_like(z)\n",
    "    return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "e960e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_content_loss = 0\n",
    "    train_cov_loss = 0\n",
    "\n",
    "    for batch_idx, data_tuple in enumerate(train_loader):\n",
    "        data = data_tuple[0]  # Extracting the data tensor\n",
    "        \n",
    "    \n",
    "        # Encoding\n",
    "        z = encoder(data)\n",
    "        \n",
    "        # Decoding\n",
    "        recon_data = decoder(z)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_data = lambda_rec * reconstruction_loss(recon_data, data, MSE=True)\n",
    "        loss_cov = lambda_cov * cov_loss(z, step=code_size)\n",
    "        loss = loss_data + loss_cov\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_content_loss += loss_data.item()\n",
    "        train_cov_loss += loss_cov.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "c8c8c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flag = True\n",
    "lr = 0.001\n",
    "scale_factor = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 1 # Latent space dimension\n",
    "\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21b0c8",
   "metadata": {},
   "source": [
    "### 2nd Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350299",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_1_weight = encoder.fc6.weight\n",
    "Latent_1_bias = encoder.fc6.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84086376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim,fixed_weights,fixed_biases):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Initialize and freeze the final layer weights and biases\n",
    "        with torch.no_grad():\n",
    "            self.fc6.weight[0,:] = nn.Parameter(fixed_weights*scale_factor)\n",
    "            self.fc6.bias[0] = nn.Parameter(fixed_biases*scale_factor)\n",
    "\n",
    "            # Freeze the parameters\n",
    "            self.fc6.weight.requires_grad = False\n",
    "            self.fc6.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 2 # Latent space dimension\n",
    "fixed_weights = Latent_1_weight\n",
    "fixed_biases = Latent_1_bias\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size,fixed_weights=fixed_weights,fixed_biases=fixed_biases)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db984dee",
   "metadata": {},
   "source": [
    "### 3rd Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_2_weight = encoder.fc6.weight\n",
    "Latent_2_bias = encoder.fc6.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_2_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0543858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim,fixed_weights,fixed_biases):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Initialize and freeze the final layer weights and biases\n",
    "        with torch.no_grad():\n",
    "            self.fc6.weight[0,:] = nn.Parameter(fixed_weights[0])\n",
    "            self.fc6.bias[0] = nn.Parameter(fixed_biases[0])\n",
    "            self.fc6.weight[1,:] = nn.Parameter(fixed_weights[1]*scale_factor)\n",
    "            self.fc6.bias[1] = nn.Parameter(fixed_biases[1]*scale_factor)\n",
    "\n",
    "            # Freeze the parameters\n",
    "            self.fc6.weight.requires_grad = False\n",
    "            self.fc6.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b64b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 3 # Latent space dimension\n",
    "fixed_weights = Latent_2_weight\n",
    "fixed_biases = Latent_2_bias\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size,fixed_weights=fixed_weights,fixed_biases=fixed_biases)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6a8e1",
   "metadata": {},
   "source": [
    "### 4th Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_3_weight = encoder.fc6.weight\n",
    "Latent_3_bias = encoder.fc6.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_3_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim,fixed_weights,fixed_biases):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Initialize and freeze the final layer weights and biases\n",
    "        with torch.no_grad():\n",
    "            self.fc6.weight[0,:] = nn.Parameter(fixed_weights[0])\n",
    "            self.fc6.bias[0] = nn.Parameter(fixed_biases[0])\n",
    "            self.fc6.weight[1,:] = nn.Parameter(fixed_weights[1])\n",
    "            self.fc6.bias[1] = nn.Parameter(fixed_biases[1])\n",
    "            self.fc6.weight[2,:] = nn.Parameter(fixed_weights[2]*scale_factor)\n",
    "            self.fc6.bias[2] = nn.Parameter(fixed_biases[2]*scale_factor)\n",
    "\n",
    "            # Freeze the parameters\n",
    "            self.fc6.weight.requires_grad = False\n",
    "            self.fc6.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e177cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 4 # Latent space dimension\n",
    "fixed_weights = Latent_3_weight\n",
    "fixed_biases = Latent_3_bias\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size,fixed_weights=fixed_weights,fixed_biases=fixed_biases)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57b904",
   "metadata": {},
   "source": [
    "### 4th Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36635713",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_4_weight = encoder.fc6.weight\n",
    "Latent_4_bias = encoder.fc6.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50707414",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_4_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55250a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim,fixed_weights,fixed_biases):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Initialize and freeze the final layer weights and biases\n",
    "        with torch.no_grad():\n",
    "            self.fc6.weight[0,:] = nn.Parameter(fixed_weights[0])\n",
    "            self.fc6.bias[0] = nn.Parameter(fixed_biases[0])\n",
    "            self.fc6.weight[1,:] = nn.Parameter(fixed_weights[1])\n",
    "            self.fc6.bias[1] = nn.Parameter(fixed_biases[1])\n",
    "            self.fc6.weight[2,:] = nn.Parameter(fixed_weights[2])\n",
    "            self.fc6.bias[2] = nn.Parameter(fixed_biases[2])\n",
    "            self.fc6.weight[3,:] = nn.Parameter(fixed_weights[3]*scale_factor)\n",
    "            self.fc6.bias[3] = nn.Parameter(fixed_biases[3]*scale_factor)\n",
    "\n",
    "            # Freeze the parameters\n",
    "            self.fc6.weight.requires_grad = False\n",
    "            self.fc6.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 5 # Latent space dimension\n",
    "fixed_weights = Latent_4_weight\n",
    "fixed_biases = Latent_4_bias\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size,fixed_weights=fixed_weights,fixed_biases=fixed_biases)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b9d6e",
   "metadata": {},
   "source": [
    "### 5th Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_5_weight = encoder.fc6.weight\n",
    "Latent_5_bias = encoder.fc6.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Latent_5_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim,fixed_weights,fixed_biases):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, (input_dim*32))  # Example dimensions\n",
    "        self.fc2 = nn.Linear((input_dim*32), (input_dim*16))\n",
    "        self.fc3 = nn.Linear((input_dim*16), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*4))\n",
    "        self.fc5 = nn.Linear((input_dim*4), (input_dim*2))\n",
    "        self.fc6 = nn.Linear((input_dim*2), latent_dim)\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Initialize and freeze the final layer weights and biases\n",
    "        with torch.no_grad():\n",
    "            self.fc6.weight[0,:] = nn.Parameter(fixed_weights[0])\n",
    "            self.fc6.bias[0] = nn.Parameter(fixed_biases[0])\n",
    "            self.fc6.weight[1,:] = nn.Parameter(fixed_weights[1])\n",
    "            self.fc6.bias[1] = nn.Parameter(fixed_biases[1])\n",
    "            self.fc6.weight[2,:] = nn.Parameter(fixed_weights[2])\n",
    "            self.fc6.bias[2] = nn.Parameter(fixed_biases[2])\n",
    "            self.fc6.weight[3,:] = nn.Parameter(fixed_weights[3])\n",
    "            self.fc6.bias[3] = nn.Parameter(fixed_biases[3])\n",
    "            self.fc6.weight[4,:] = nn.Parameter(fixed_weights[4]*scale_factor)\n",
    "            self.fc6.bias[4] = nn.Parameter(fixed_biases[4]*scale_factor)\n",
    "\n",
    "            # Freeze the parameters\n",
    "            self.fc6.weight.requires_grad = False\n",
    "            self.fc6.bias.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc6(x)  # No activation here, latent representation\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, (input_dim*2))\n",
    "        self.fc2 = nn.Linear((input_dim*2), (input_dim*4))\n",
    "        self.fc3 = nn.Linear((input_dim*4), (input_dim*8))\n",
    "        self.fc4 = nn.Linear((input_dim*8), (input_dim*16))\n",
    "        self.fc5 = nn.Linear((input_dim*16), (input_dim*32))\n",
    "        self.fc6 = nn.Linear((input_dim*32), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.LeakyReLU(0.2)  # Add this line\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.relu(self.fc2(z))\n",
    "        z = self.relu(self.fc3(z))\n",
    "        z = self.relu(self.fc4(z))\n",
    "        z = self.relu(self.fc5(z))\n",
    "        z = self.sigmoid(self.fc6(z))  # Sigmoid activation on final layer\n",
    "        return z # Define the reconstruction loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(VIX_TS.iloc[:,1:].values, dtype=torch.float32)\n",
    "\n",
    "# Define hyperparameters (unchanged)\n",
    "num_epochs = 100\n",
    "lambda_rec = 1.0\n",
    "lambda_cov = 0.1\n",
    "\n",
    "# Initialize encoder and decoder models\n",
    "input_dim = X_train_tensor.shape[1]  # Number of features in your data\n",
    "code_size = 6 # Latent space dimension\n",
    "fixed_weights = Latent_5_weight\n",
    "fixed_biases = Latent_5_bias\n",
    "output_dim = input_dim  # Assuming the output dimension is same as input\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, latent_dim=code_size,fixed_weights=fixed_weights,fixed_biases=fixed_biases)\n",
    "decoder = Decoder(latent_dim=code_size, output_dim=output_dim)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=Flag)\n",
    "\n",
    "# Define the optimizer (unchanged)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Training loop (unchanged)\n",
    "for epoch in range(num_epochs):\n",
    "    train_PCA_AE(encoder, decoder, optimizer, epoch, train_loader, lambda_rec, lambda_cov, code_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d64d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the encoder is in evaluation mode\n",
    "encoder.eval()\n",
    "\n",
    "# Assuming X_full_tensor is your entire dataset as a PyTorch tensor\n",
    "X_full_tensor = torch.tensor(VIX_TS.iloc[:, 1:].values, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for the entire dataset\n",
    "full_dataset = TensorDataset(X_full_tensor)\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size)  # Adjust batch_size as needed\n",
    "\n",
    "# Process the entire dataset through the encoder only\n",
    "encoded_data = []\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for data_tuple in full_loader:\n",
    "        data = data_tuple[0]\n",
    "        encoded = encoder(data)\n",
    "        encoded_data.append(encoded)\n",
    "\n",
    "# Convert the list of batches into a single tensor\n",
    "encoded_data_tensor = torch.cat(encoded_data, dim=0)\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "encoded_data_np = encoded_data_tensor.numpy()\n",
    "\n",
    "# Create a DataFrame from the NumPy array\n",
    "encoded_data_df = pd.DataFrame(encoded_data_np, columns=['Level', 'Slope', 'Curve','AE 4','AE 5','AE 6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0693c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(encoded_data_df)\n",
    "scaled_dfx = pd.DataFrame(data=scaled)\n",
    "scaled_dfx['Date']=VIX_TS['DATE']\n",
    "scaled_dfx = scaled_dfx[['Date',0,1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159febeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_PCAAE = scaled_dfx[['Date',1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_PCAAE.to_csv('/Users/juliusraschke/Documents/Quantitative Finance/Summer Semester 2/Advanced Quant Finance/Data/OUR_PCAAE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = pd.read_csv(DATA_PATH + '/OUR_PCA.csv')\n",
    "PCA = PCA.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = scaled_dfx.iloc[:,1:]\n",
    "column_mapping = {\n",
    "    0: 'Level',\n",
    "    1: 'Slope',\n",
    "    2: 'Curve',\n",
    "    3: 'PC4',\n",
    "    4: 'PC5',\n",
    "    5: 'PC6',\n",
    "}\n",
    "AE.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = PCA.corrwith(AE)\n",
    "\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the variables\n",
    "variables = ['Level', 'Slope', 'Curve', 'PC4', 'PC5', 'PC6']\n",
    "\n",
    "# Create a figure with subplots\n",
    "plt.figure(figsize=(15, 10)) # Adjust the size as needed\n",
    "\n",
    "# Loop through each variable and create a subplot\n",
    "for i, var in enumerate(variables, 1):\n",
    "    plt.subplot(2, 3, i) # 2 rows, 3 columns of subplots\n",
    "    plt.scatter(PCA[var], -AE[var])\n",
    "    plt.xlabel(f'PCA - {var}')\n",
    "    plt.ylabel(f'AE - {var}')\n",
    "    plt.title(f'Scatter Plot for {var}')\n",
    "\n",
    "# Adjust layout for better viewing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54842f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
